{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cours 5 : Régression, Cross-Validation, Pénalisation (TP 9-10)<span class=\"tocSkip\"></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cours, nous présenterons la régression linéaire sous son aspect bayésien et ses extensions. <br>\n",
    "\n",
    "La première partie traite de la régression linéaire sous son approche habituelle, vous entraîne à l'implémenter, vous initie à la validation de modèle et propose des exercices pour vous entraîner.<br>\n",
    "\n",
    "La seconde partie traite des différentes pénalisations de la régression linéaire. Les extensions du type LASSO et RIDGE seront explorées. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Modèle-linéaire\" data-toc-modified-id=\"Modèle-linéaire-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Modèle linéaire</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercice\" data-toc-modified-id=\"Exercice-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Exercice</a></span></li></ul></li><li><span><a href=\"#Validation\" data-toc-modified-id=\"Validation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Validation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercice\" data-toc-modified-id=\"Exercice-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Exercice</a></span></li></ul></li><li><span><a href=\"#Pénalisation\" data-toc-modified-id=\"Pénalisation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pénalisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ridge\" data-toc-modified-id=\"Ridge-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Ridge</a></span></li><li><span><a href=\"#Lasso\" data-toc-modified-id=\"Lasso-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Lasso</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Modèle linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Le modèle générateur est de la forme $Y_t = X_t \\theta + \\varepsilon$ <br>\n",
    "$\\varepsilon_t$ iid $\\sim \\mathcal{N}(0,\\sigma^2)$ <br>\n",
    "On recherche le paramètre $\\theta$ sur notre jeu de données en minimisant la fonction de perte :<br>\n",
    "\n",
    "\\begin{equation*}\n",
    " L(\\theta) = || Y - X\\theta||^2_2\n",
    "\\end{equation*}\n",
    "\n",
    "Après quelques calculs (dérivée = 0), on trouve que :\n",
    "\\begin{equation*}\n",
    "\\hat{\\theta} = (X^TX)^{-1}X^TY\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Implémenter une fonction compute_estimator() qui calcule l'estimateur d'un modèle linéaire en prenant en entrée X et Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(2)\n",
    "theta_0 = 3\n",
    "theta_1 = 0.5\n",
    "X = np.linspace(1, 100, num = 50)\n",
    "epsilon = np.random.normal(0,1, size=50)\n",
    "\n",
    "Y=theta_0 + theta_1*x+epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_estimator(X,Y):\n",
    "    ##Votre code\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) En utilisant l'estimateur calculé, prédire Y_prime en utilisant X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "X_prime = np.random.uniform(low=0, high=10, size=(100,))\n",
    "def prediction(X_prime,theta):\n",
    "    ##Votre code\n",
    "    return Y_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Comparer vos résultats en utilisant LinearRegression de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Charger la table cours_5_data.csv. Prenez X = \"Item_Stock\" et Y = \"Item_Outlet_Sales\", calculer l'estimateur $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la majorité des cas, nous sommes confrontés à des problèmes de prédiction où il y a une base d'entraînement et une base de test pour pouvoir mesurer et \"valider\" la qualité de notre prédiction. Cette bonne pratique permet d'éviter un overfitting sur l'échantillon d'apprentissage, en vérifiant le score sur l'échantillon test, les deux doivent être similaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Trouver une fonction dans scikit-learn qui sépare en 2 échantillons la table cours_5_data.csv, séparez cette table en 67% apprentissage et 33% test. Utilisez 2 comme seed. <br>\n",
    "2) Construire une fonction erreur_quadratique_moyenne() pour mesurer l'erreur sur la base test entre vos prédictions et la valeur réelle de Y. Faites de même en prédisant le Y de la base d'apprentissage et regardez l'erreur quadratique moyenne. Qu'en déduisez-vous ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pénalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les régressions pénalisés sont des techniques puissantes généralement utilisées pour créer des modèles parcimonieux en présence d'un \"grand\" nombre de variables :<br>\n",
    "\n",
    "- Suffisamment grand pour accroître la tendance d'un modèle à l'overfitting. \n",
    "- Assez grand pour causer des problèmes de calcul (problème d'inversion de matrice). <br>\n",
    "\n",
    "Nous allons présenter deux types de pénalisations appelées Ridge et Lasso. <br>\n",
    "La principale différence réside dans la façon dont ils attribuent les pénalités aux coefficients. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pénalise la fonction de perte avec un terme supplémentaire qui contraint de façon quadratique le paramètre $\\theta$: \\begin{equation*}\n",
    " L(\\theta,\\lambda) = || Y - X\\theta||^2_2 + \\lambda ||\\theta||^2_2\n",
    "\\end{equation*}\n",
    "\n",
    "Trouver le paramètre $\\hat{\\theta}$ qui minimise la fonction de perte.\n",
    "#explication ridge tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Le Lasso pénalise la fonction de perte en essayant de neutraliser au maximum l'accroissement absolu des variables : \n",
    "\n",
    "\\begin{equation*}\n",
    " L(\\theta,\\lambda) = || Y - X\\theta||^2_2 + \\lambda ||\\theta||^2_1\n",
    "\\end{equation*}\n",
    "#explication tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouvez-les fonctions Ridge et Lasso dans scikit-learn, amusez-vous avec le jeu de données et comparer les erreurs quadratiques moyennes de chaque méthode, à la fois sur l'échantillon d'apprentissage et l'échantillon de test. Par ailleurs, comparer aussi les coefficients estimés entre les 3 méthodes (regression simple, penalisation Ridge, penalisation Lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
