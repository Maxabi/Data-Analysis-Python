{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cours 5 : Régression, Cross-Validation, Pénalisation (TP 9-10)<span class=\"tocSkip\"></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce cours, nous présenterons la régression linéaire sous son aspect bayésien et ses extensions. <br>\n",
    "\n",
    "La première partie traite de la régression linéaire sous son approche habituelle, vous entraîne à l'implémenter, vous initie à la validation de modèle et propose des exercices pour vous entraîner.<br>\n",
    "\n",
    "La seconde partie traite des différentes pénalisations de la régression linéaire. Les extensions du type LASSO et RIDGE seront explorées. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Modèle-linéaire\" data-toc-modified-id=\"Modèle-linéaire-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Modèle linéaire</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercice\" data-toc-modified-id=\"Exercice-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Exercice</a></span></li></ul></li><li><span><a href=\"#Validation\" data-toc-modified-id=\"Validation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Validation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercice\" data-toc-modified-id=\"Exercice-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Exercice</a></span></li></ul></li><li><span><a href=\"#Pénalisation\" data-toc-modified-id=\"Pénalisation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Pénalisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ridge\" data-toc-modified-id=\"Ridge-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Ridge</a></span></li><li><span><a href=\"#Lasso\" data-toc-modified-id=\"Lasso-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Lasso</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Modèle linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Le modèle générateur est de la forme $Y_t = X_t \\theta + \\varepsilon$ <br>\n",
    "$\\varepsilon_t$ iid $\\sim \\mathcal{N}(0,\\sigma^2)$ <br>\n",
    "On recherche le paramètre $\\theta$ sur notre jeu de données en minimisant la fonction de perte :<br>\n",
    "\n",
    "\\begin{equation*}\n",
    " L(\\theta) = || Y - X\\theta||^2_2\n",
    "\\end{equation*}\n",
    "\n",
    "Après quelques calculs (dérivée = 0), on trouve que :\n",
    "\\begin{equation*}\n",
    "\\hat{\\theta} = (X^TX)^{-1}X^TY\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Implémenter une fonction compute_estimator() qui calcule l'estimateur d'un modèle linéaire en prenant en entrée X et Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(2)\n",
    "theta_0 = 3\n",
    "theta_1 = 0.5\n",
    "X = np.linspace(1, 100, num = 50)\n",
    "epsilon = np.random.normal(0,1, size=50)\n",
    "\n",
    "Y=theta_0 + theta_1*X+epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_estimator(X,Y):\n",
    "    X = X.reshape(len(X),1)\n",
    "    tmp = np.ones(shape=(len(X),1))\n",
    "    print(X.shape)\n",
    "    print(tmp.shape)\n",
    "    X= np.concatenate((tmp,X),axis=1)\n",
    "    Y = Y.reshape(len(Y),1)\n",
    "    \n",
    "    ##Votre code\n",
    "    t1 = np.linalg.inv(np.dot(np.transpose(X),X))\n",
    "    t2 = np.dot(np.transpose(X),Y)\n",
    "    return np.dot(t1,t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n",
      "(50, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.54979832],\n",
       "       [0.50614768]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_estimator(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLORIE +1, LEA +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) En utilisant l'estimateur calculé, prédire Y_prime en utilisant X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "X_prime = np.random.uniform(low=0, high=10, size=(100,))\n",
    "def prediction(X_prime,theta):\n",
    "    ##Votre code\n",
    "    Y_prime = theta[0]+X_prime*theta[1]\n",
    "    return Y_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n",
      "(50, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.75657639, 2.68102334, 5.33190219, 4.7531725 , 4.67748019,\n",
       "       4.22178034, 3.58562263, 5.68422393, 4.06649349, 3.90033838,\n",
       "       5.69365279, 5.22803874, 3.23097159, 5.14926205, 3.48333642,\n",
       "       6.52475393, 6.87217443, 5.05136659, 6.83464962, 2.95292205,\n",
       "       5.10708967, 2.88024445, 4.71672954, 3.03838731, 3.19341556,\n",
       "       5.57021084, 3.69375281, 3.09110142, 3.66487307, 4.32043594,\n",
       "       4.91749381, 3.57091697, 5.79120209, 4.99484507, 5.10704224,\n",
       "       4.50804649, 6.56677586, 5.485476  , 3.37126891, 6.09664005,\n",
       "       7.43185121, 5.08057903, 7.05208346, 4.27886789, 5.42038515,\n",
       "       4.71381228, 4.76038445, 6.4803346 , 5.2607464 , 7.37714245,\n",
       "       5.30429528, 2.96531986, 4.40403188, 6.85635838, 4.60615001,\n",
       "       2.68748246, 3.80088017, 2.88964799, 7.58015719, 7.46236803,\n",
       "       6.60028738, 5.5958817 , 6.42162489, 3.40632899, 4.0329286 ,\n",
       "       5.20235064, 4.35484384, 2.78100134, 7.52600665, 4.78370599,\n",
       "       5.10078484, 4.18739518, 3.86449035, 4.50803249, 6.76103253,\n",
       "       6.27882643, 4.4691638 , 2.61568527, 6.5858449 , 3.91330346,\n",
       "       5.49904435, 2.67912382, 5.90151846, 4.51123914, 5.06572581,\n",
       "       4.64983458, 4.3257283 , 5.33856019, 7.47416318, 3.12061251,\n",
       "       4.13534909, 2.76135646, 6.28719155, 5.877782  , 3.63617217,\n",
       "       4.65918618, 5.80858932, 5.89787069, 3.41266437, 7.01226063])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction(X_prime,compute_estimator(X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Comparer vos résultats en utilisant LinearRegression de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/LF/anaconda/lib/python3.6/site-packages/sklearn/linear_model/base.py:509: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.75657639, 2.68102334, 5.33190219, 4.7531725 , 4.67748019,\n",
       "       4.22178034, 3.58562263, 5.68422393, 4.06649349, 3.90033838,\n",
       "       5.69365279, 5.22803874, 3.23097159, 5.14926205, 3.48333642,\n",
       "       6.52475393, 6.87217443, 5.05136659, 6.83464962, 2.95292205,\n",
       "       5.10708967, 2.88024445, 4.71672954, 3.03838731, 3.19341556,\n",
       "       5.57021084, 3.69375281, 3.09110142, 3.66487307, 4.32043594,\n",
       "       4.91749381, 3.57091697, 5.79120209, 4.99484507, 5.10704224,\n",
       "       4.50804649, 6.56677586, 5.485476  , 3.37126891, 6.09664005,\n",
       "       7.43185121, 5.08057903, 7.05208346, 4.27886789, 5.42038515,\n",
       "       4.71381228, 4.76038445, 6.4803346 , 5.2607464 , 7.37714245,\n",
       "       5.30429528, 2.96531986, 4.40403188, 6.85635838, 4.60615001,\n",
       "       2.68748246, 3.80088017, 2.88964799, 7.58015719, 7.46236803,\n",
       "       6.60028738, 5.5958817 , 6.42162489, 3.40632899, 4.0329286 ,\n",
       "       5.20235064, 4.35484384, 2.78100134, 7.52600665, 4.78370599,\n",
       "       5.10078484, 4.18739518, 3.86449035, 4.50803249, 6.76103253,\n",
       "       6.27882643, 4.4691638 , 2.61568527, 6.5858449 , 3.91330346,\n",
       "       5.49904435, 2.67912382, 5.90151846, 4.51123914, 5.06572581,\n",
       "       4.64983458, 4.3257283 , 5.33856019, 7.47416318, 3.12061251,\n",
       "       4.13534909, 2.76135646, 6.28719155, 5.877782  , 3.63617217,\n",
       "       4.65918618, 5.80858932, 5.89787069, 3.41266437, 7.01226063])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X.reshape(len(X),1),Y)\n",
    "lr.predict(X_prime.reshape(len(X_prime),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Charger la table cours_5_data.csv. Prenez X = \"Item_Stock\" et Y = \"Item_Outlet_Sales\", calculer l'estimateur $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/cours_5_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Item_Stock\"]\n",
    "y = df[\"Item_Outlet_Sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7060, 1)\n",
      "(7060, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-36.26504787],\n",
       "       [ 15.25687788]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_estimator(X.values,y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([249.8092,  48.2692, 141.618 , ...,  85.1224, 103.1332,  75.467 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X.values.reshape(X.shape[0],1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coef : [15.25687788]\n",
      "intercept : -36.26504786675196\n"
     ]
    }
   ],
   "source": [
    "print(\"coef : \"+str(lr.coef_))\n",
    "print(\"intercept : \"+str(lr.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la majorité des cas, nous sommes confrontés à des problèmes de prédiction où il y a une base d'entraînement et une base de test pour pouvoir mesurer et \"valider\" la qualité de notre prédiction. Cette bonne pratique permet d'éviter un overfitting sur l'échantillon d'apprentissage, en vérifiant le score sur l'échantillon test, les deux doivent être similaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Trouver une fonction dans scikit-learn qui sépare en 2 échantillons la table cours_5_data.csv, séparez cette table en 67% apprentissage et 33% test. Utilisez 2 comme seed. <br>\n",
    "2) Construire une fonction erreur_quadratique_moyenne() pour mesurer l'erreur sur la base test entre vos prédictions et la valeur réelle de Y. Faites de même en prédisant le Y de la base d'apprentissage et regardez l'erreur quadratique moyenne. Qu'en déduisez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Stock</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.30</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>1999</td>\n",
       "      <td>3735.1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.92</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>48.2692</td>\n",
       "      <td>2009</td>\n",
       "      <td>443.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.50</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>141.6180</td>\n",
       "      <td>1999</td>\n",
       "      <td>2097.2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>182.0950</td>\n",
       "      <td>1998</td>\n",
       "      <td>732.3800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.8614</td>\n",
       "      <td>1987</td>\n",
       "      <td>994.7052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Item_Weight  Item_Visibility  Item_Stock  Outlet_Establishment_Year  \\\n",
       "0         9.30         0.016047    249.8092                       1999   \n",
       "1         5.92         0.019278     48.2692                       2009   \n",
       "2        17.50         0.016760    141.6180                       1999   \n",
       "3        19.20         0.000000    182.0950                       1998   \n",
       "4         8.93         0.000000     53.8614                       1987   \n",
       "\n",
       "   Item_Outlet_Sales  \n",
       "0          3735.1380  \n",
       "1           443.4228  \n",
       "2          2097.2700  \n",
       "3           732.3800  \n",
       "4           994.7052  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"Item_Stock\",\"Item_Weight\",\"Item_Visibility\"]]\n",
    "y = df[\"Item_Outlet_Sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4730, 3), (2330, 3))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def erreur_quadratique_moyenne(y_pred,y_true): #MSE\n",
    "    return(np.mean(np.square(y_pred-y_true)))\n",
    "def rac_erreur_quadratique_moyenne(y_pred,y_true): #RMSE\n",
    "    return (np.sqrt(np.mean(np.square(y_pred-y_true))))\n",
    "def erreur_absolue_moyenne(y_pred,y_true): #MAE\n",
    "    return (np.mean(np.abs(y_pred-y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(X_train.values.reshape(X_train.shape[0],3),y_train)\n",
    "y_pred=lr.predict(X_test.values.reshape(X_test.shape[0],3))\n",
    "y_pred_train=lr.predict(X_train.values.reshape(X_train.shape[0],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eam_test=erreur_absolue_moyenne(y_pred,y_test)\n",
    "raqm_test=rac_erreur_quadratique_moyenne(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "raqm_train=rac_erreur_quadratique_moyenne(y_pred_train,y_train)\n",
    "eam_train=erreur_absolue_moyenne(y_pred_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE test : 868.5290638051998, RMSE test : 1177.7954799869544, MAE train : 1204.1761486416638, RMSE train 890.6697431565144\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE test : {}, RMSE test : {}, MAE train : {}, RMSE train {}\".format(eam_test,raqm_test,raqm_train,eam_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE test : 872.2943867353413, RMSE test : 1184.7665887590717, MAE train : 1210.5447621332578, RMSE train 890.0802803874024\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE test : {}, RMSE test : {}, MAE train : {}, RMSE train {}\".format(eam_test,raqm_test,raqm_train,eam_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE test : 871.3421860903337, RMSE test : 1183.920033957123, MAE train : 1210.6721292543189, RMSE train 890.0865017575998\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE test : {}, RMSE test : {}, MAE train : {}, RMSE train {}\".format(eam_test,raqm_test,raqm_train,eam_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pénalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les régressions pénalisés sont des techniques puissantes généralement utilisées pour créer des modèles parcimonieux en présence d'un \"grand\" nombre de variables :<br>\n",
    "\n",
    "- Suffisamment grand pour accroître la tendance d'un modèle à l'overfitting. \n",
    "- Assez grand pour causer des problèmes de calcul (problème d'inversion de matrice). <br>\n",
    "\n",
    "Nous allons présenter deux types de pénalisations appelées Ridge et Lasso. <br>\n",
    "La principale différence réside dans la façon dont ils attribuent les pénalités aux coefficients. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pénalise la fonction de perte avec un terme supplémentaire qui contraint de façon quadratique le paramètre $\\theta$: \\begin{equation*}\n",
    " L(\\theta,\\lambda) = || Y - X\\theta||^2_2 + \\lambda ||\\theta||^2_2\n",
    "\\end{equation*}\n",
    "\n",
    "Trouver le paramètre $\\hat{\\theta}$ qui minimise la fonction de perte.\n",
    "#explication ridge tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Le Lasso pénalise la fonction de perte en essayant de neutraliser au maximum l'accroissement absolu des variables : \n",
    "\n",
    "\\begin{equation*}\n",
    " L(\\theta,\\lambda) = || Y - X\\theta||^2_2 + \\lambda ||\\theta||^2_1\n",
    "\\end{equation*}\n",
    "#explication tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouvez-les fonctions Ridge et Lasso dans scikit-learn, amusez-vous avec le jeu de données et comparer les erreurs de chaque méthode, à la fois sur l'échantillon d'apprentissage et l'échantillon de test. Par ailleurs, comparer aussi les coefficients estimés entre les 3 méthodes (regression simple, penalisation Ridge, penalisation Lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=load_boston()[\"target\"]\n",
    "X=load_boston()[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1=X+np.random.normal(0,1)\n",
    "X_2=X+np.random.normal(0,1)\n",
    "X_3=X+np.random.normal(0,1)\n",
    "X_4=X+np.random.normal(0,1)\n",
    "X_5=X+np.random.normal(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.concatenate([X_1,X_2,X_3,X_4,X_5],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=X[:60,:]\n",
    "y=y[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = df[[\"Item_Stock\",\"Item_Weight\",\"Item_Visibility\",\"Outlet_Establishment_Year\"]]\n",
    "#y = df[\"Item_Outlet_Sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = [0.000001,0.001,0.01,0.1,0.5,1,5,10,15,20,30,50,100,500,1000,2000,5000,10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_2=np.linspace(0.7,1.4,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 0.7 MAE : 1.4416166707626146\n",
      "alpha : 0.7368421052631579 MAE : 1.442478267196894\n",
      "alpha : 0.7736842105263158 MAE : 1.4432796035819409\n",
      "alpha : 0.8105263157894737 MAE : 1.44402826210578\n",
      "alpha : 0.8473684210526315 MAE : 1.4447305564571213\n",
      "alpha : 0.8842105263157894 MAE : 1.4453917875620939\n",
      "alpha : 0.9210526315789473 MAE : 1.4460164398750626\n",
      "alpha : 0.9578947368421051 MAE : 1.4466083336866615\n",
      "alpha : 0.9947368421052631 MAE : 1.4471707445011117\n",
      "alpha : 1.031578947368421 MAE : 1.44770649756904\n",
      "alpha : 1.068421052631579 MAE : 1.448565759054484\n",
      "alpha : 1.1052631578947367 MAE : 1.4494911329374247\n",
      "alpha : 1.1421052631578945 MAE : 1.4503928133512651\n",
      "alpha : 1.1789473684210525 MAE : 1.4512724896236482\n",
      "alpha : 1.2157894736842105 MAE : 1.4521316595774996\n",
      "alpha : 1.2526315789473683 MAE : 1.4529716568817121\n",
      "alpha : 1.289473684210526 MAE : 1.4537936737924166\n",
      "alpha : 1.3263157894736841 MAE : 1.4545987802094633\n",
      "alpha : 1.3631578947368421 MAE : 1.4553879397109655\n",
      "alpha : 1.4 MAE : 1.4561620230848848\n"
     ]
    }
   ],
   "source": [
    "for alp in alphas_2:\n",
    "    ridge=Ridge(alpha=alp)\n",
    "    ridge.fit(X_train,y_train)\n",
    "    y_pred=ridge.predict(X_train)\n",
    "    print(\"alpha : \"+str(alp)+\" MAE : \"+str(erreur_absolue_moyenne(y_pred,y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 1e-06 MAE : 1.291254136678082\n",
      "alpha : 0.001 MAE : 1.288830640869778\n",
      "alpha : 0.01 MAE : 1.450108707061522\n",
      "alpha : 0.1 MAE : 1.5450506285397971\n",
      "alpha : 0.5 MAE : 2.017404050550995\n",
      "alpha : 1 MAE : 2.2871676679738533\n",
      "alpha : 5 MAE : 2.7600904494669525\n",
      "alpha : 10 MAE : 2.935666807003961\n",
      "alpha : 15 MAE : 3.0594186892609274\n",
      "alpha : 20 MAE : 3.1427673071434326\n",
      "alpha : 30 MAE : 3.318095390541842\n",
      "alpha : 50 MAE : 3.739507371523281\n",
      "alpha : 100 MAE : 4.606056863027363\n",
      "alpha : 500 MAE : 5.188\n",
      "alpha : 1000 MAE : 5.188\n",
      "alpha : 2000 MAE : 5.188\n",
      "alpha : 5000 MAE : 5.188\n",
      "alpha : 10000 MAE : 5.188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/LF/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "for alp in alphas:\n",
    "    lasso=Lasso(alpha=alp)\n",
    "    lasso.fit(X_train,y_train)\n",
    "    y_pred=lasso.predict(X_train)\n",
    "    print(\"alpha : \"+str(alp)+\" MAE : \"+str(erreur_absolue_moyenne(y_pred,y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_ridge = 0.1\n",
    "alpha_lasso = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge=Ridge(alpha=alpha_ridge)\n",
    "ridge.fit(X_train,y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "lasso=Lasso(alpha=alpha_lasso)\n",
    "lasso.fit(X_train,y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "lr=LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "y_pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge 2.8788922155716414\n",
      "lasso 2.672195817951981\n",
      "linear reg 2.732334098815918\n"
     ]
    }
   ],
   "source": [
    "print(\"ridge \"+str(erreur_absolue_moyenne(y_pred_ridge,y_test)))\n",
    "print(\"lasso \"+str(erreur_absolue_moyenne(y_pred_lasso,y_test)))\n",
    "print(\"linear reg \"+str(erreur_absolue_moyenne(y_pred_lr,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear reg 1.620961680556789\n"
     ]
    }
   ],
   "source": [
    "print(\"linear reg \"+str(erreur_absolue_moyenne(y_pred_lr,y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
